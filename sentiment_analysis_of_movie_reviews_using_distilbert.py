# -*- coding: utf-8 -*-
"""Sentiment Analysis of Movie Reviews Using DistilBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QxRyS4Hr9eFfkkb-Gf6lfK0H8iAH4CaT
"""

!pip install transformers

from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")

# Tokenize input text
inputs = tokenizer("I love using Hugging Face Transformers!", return_tensors="pt")

import torch

# Perform inference
with torch.no_grad():
    outputs = model(**inputs)

# Get predicted class
logits = outputs.logits
predicted_class = torch.argmax(logits, dim=1).item()

print(f"Predicted class: {predicted_class}")

!pip install datasets

from datasets import load_dataset

# Load the dataset
dataset = load_dataset("imdb")
print(dataset)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./results",          # Directory to save model and logs
    evaluation_strategy="epoch",     # Evaluate at the end of each epoch
    learning_rate=2e-5,              # Learning rate for the optimizer
    per_device_train_batch_size=8,  # Batch size for training
    per_device_eval_batch_size=8,   # Batch size for evaluation
    num_train_epochs=1,              # Number of epochs to train
    weight_decay=0.01,               # Weight decay for regularization
)

trainer = Trainer(
    model=model,                      # The pre-trained model
    args=training_args,               # Training arguments
    train_dataset=tokenized_datasets["train"],  # Training dataset
    eval_dataset=tokenized_datasets["test"],    # Evaluation dataset
)

import torch
print(torch.cuda.is_available())

trainer.train()

eval_results = trainer.evaluate()
print(eval_results)

trainer.save_model("./fine-tuned-model")

tokenizer.save_pretrained("./fine-tuned-model")

from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("./fine-tuned-model")
model = AutoModelForSequenceClassification.from_pretrained("./fine-tuned-model")

# Function to predict sentiment
def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    predicted_class = torch.argmax(logits, dim=1).item()
    return "Positive" if predicted_class == 1 else "Negative"

# Test with more example texts
texts = [
    "This movie was fantastic! I loved every moment.",
    "I didn't like the plot at all. It was boring.",
    "The acting was great, but the story was weak.",
    "Absolutely wonderful! I would watch it again.",
    "This movie will be great for kids but I didn't like it"
]

for text in texts:
    sentiment = predict_sentiment(text)
    print(f"Text: \"{text}\" - Sentiment: {sentiment}")